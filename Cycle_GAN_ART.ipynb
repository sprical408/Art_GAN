{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Cycle_GAN_ART.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMzyRyCGrAllUW37yOm2s00"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6Ss2maicwiB"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./mount')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ASzVcfcXX4v"
      },
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision.utils import make_grid\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import Resize, RandomCrop, Normalize, ToTensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvLNi1OQLvhZ"
      },
      "source": [
        "# check if CUDA is available\n",
        "# if yes, set default tensor type to cuda\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "  print(\"using cuda:\", torch.cuda.get_device_name(0))\n",
        "  \n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmQkEuf3BliV"
      },
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_dir, size=(256, 256)):\n",
        "        super().__init__()\n",
        "        self.image_dir = image_dir\n",
        "        self.img_idx = dict()\n",
        "\n",
        "        # 전처리 과정\n",
        "        self.transform = transforms.Compose([\n",
        "            Resize((286, 286)),\n",
        "            RandomCrop((256,256)),\n",
        "            ToTensor(),\n",
        "            Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
        "        ])\n",
        "\n",
        "        \n",
        "        for i, fl in enumerate(os.listdir(self.image_dir)):\n",
        "            self.img_idx[i] = fl\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_dir = os.path.join(self.image_dir, self.img_idx[idx])\n",
        "        img = Image.open(image_dir)\n",
        "        img = self.transform(img)\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zd6eG1RaB_Ur"
      },
      "source": [
        "def unnormalize(image, mean_=0.5, std_=0.5):\n",
        "    if torch.is_tensor(image):\n",
        "        image = image.detach().cpu().numpy()\n",
        "    un_normalized_img = image * std_ + mean_\n",
        "    un_normalized_img = un_normalized_img * 255\n",
        "    return np.uint8(un_normalized_img)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7Kip3GQB-pR"
      },
      "source": [
        "# Load Dataset\n",
        "photo_ds = ImageDataset('/content/mount/MyDrive/dataset/Art_GAN/photo/')\n",
        "monet_ds = ImageDataset('/content/mount/MyDrive/dataset/Art_GAN/vangogh/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM3FqawBB_di"
      },
      "source": [
        "photo_dl = DataLoader(photo_ds, batch_size = 1, shuffle = True, pin_memory=True)\n",
        "monet_dl = DataLoader(monet_ds, batch_size = 1, shuffle = True, pin_memory=True)\n",
        "test = DataLoader(photo_ds, batch_size = 2, shuffle = False, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z991RFWaXsE1"
      },
      "source": [
        "def show_test(fixed_X, G_XtoY, mean_=0.5, std_=0.5):\n",
        "    #Create fake pictures for both cycles\n",
        "    fake_Y = G_XtoY(fixed_X.to(device))\n",
        "    \n",
        "    #Generate grids\n",
        "    grid_x =  make_grid(fixed_X).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    grid_fake_y =  make_grid(fake_Y).permute(1, 2, 0).detach().cpu().numpy()\n",
        "    \n",
        "    #Normalize pictures to pixel range rom 0 to 255\n",
        "    X, fake_Y = unnormalize(grid_x, mean_, std_), unnormalize(grid_fake_y, mean_, std_)\n",
        "    \n",
        "    #Transformation from X -> Y\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, sharey=True, figsize=(20, 10))\n",
        "    ax1.imshow(X)\n",
        "    ax1.axis('off')\n",
        "    ax1.set_title('Original')\n",
        "    ax2.imshow(fake_Y)\n",
        "    ax2.axis('off')\n",
        "    ax2.set_title('Converted')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iRQAHS9QXnX"
      },
      "source": [
        "class Resblock(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.ReflectionPad2d(1),\n",
        "        nn.Conv2d(256, 256, kernel_size = 3, bias = False),\n",
        "        nn.InstanceNorm2d(256)\n",
        "    )\n",
        "\n",
        "    self.conv2 = nn.Sequential(\n",
        "        nn.ReflectionPad2d(1),\n",
        "        nn.Conv2d(256, 256, kernel_size = 3, bias = False),\n",
        "        nn.InstanceNorm2d(256)\n",
        "    )\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    output = torch.nn.functional.relu(self.conv1(inputs))\n",
        "    return torch.nn.functional.relu(inputs + self.conv2(output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFXbn9r4Q-MV"
      },
      "source": [
        "# Create Generator\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.ReflectionPad2d(3),\n",
        "        nn.Conv2d(3, 64, kernel_size = 7, bias = False),\n",
        "        nn.InstanceNorm2d(64),\n",
        "        nn.GELU()\n",
        "    )\n",
        "\n",
        "    self.downsampling = nn.Sequential(\n",
        "        nn.Conv2d(64, 128, kernel_size = 3, stride = 2, padding = 1, bias = False),\n",
        "        nn.InstanceNorm2d(128),\n",
        "        nn.GELU(),\n",
        "\n",
        "        nn.Conv2d(128, 256, kernel_size = 3, stride = 2, padding = 1, bias = False),\n",
        "        nn.InstanceNorm2d(256),\n",
        "        nn.GELU()\n",
        "    )\n",
        "\n",
        "    resblock_layer = []\n",
        "    for i in range(9):\n",
        "      resblock_layer += [Resblock()]\n",
        "\n",
        "    self.resblock = nn.Sequential(*resblock_layer)\n",
        "\n",
        "    self.upsampling = nn.Sequential(\n",
        "        nn.ConvTranspose2d(256, 128, kernel_size = 3, stride = 2, padding = 1, output_padding = 1, bias = False),\n",
        "        nn.InstanceNorm2d(128),\n",
        "        nn.GELU(),\n",
        "\n",
        "        nn.ConvTranspose2d(128, 64, kernel_size = 3, stride = 2, padding = 1, output_padding = 1, bias = False),\n",
        "        nn.InstanceNorm2d(64),\n",
        "        nn.GELU()\n",
        "    )\n",
        "\n",
        "    self.conv2 = nn.Sequential(\n",
        "        nn.ReflectionPad2d(3),\n",
        "        nn.Conv2d(64, 3, kernel_size = 7, bias = False),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "    \n",
        "\n",
        "  def forward(self, inputs):\n",
        "    output = self.conv1(inputs)\n",
        "\n",
        "    output = self.downsampling(output)\n",
        "    output = self.resblock(output)\n",
        "    output = self.upsampling(output)\n",
        "    output = self.conv2(output)\n",
        "    \n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9X5upGVCDF5"
      },
      "source": [
        "# CycleGAN - Discriminator with PatchGAN\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # Set simple model\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Conv2d(3, 64, kernel_size = 4, padding = 1, stride = 2),\n",
        "        nn.GELU(),\n",
        "\n",
        "        nn.Conv2d(64, 128, kernel_size = 4, padding = 1, stride = 2, bias = False),\n",
        "        nn.InstanceNorm2d(128),\n",
        "        nn.GELU(),\n",
        "\n",
        "        nn.Conv2d(128, 256, kernel_size = 4, padding = 1, stride = 2, bias = False),\n",
        "        nn.InstanceNorm2d(256),\n",
        "        nn.GELU(),\n",
        "\n",
        "        nn.Conv2d(256, 512, kernel_size = 4, padding = 1, stride = 2, bias = False),\n",
        "        nn.InstanceNorm2d(512),\n",
        "        nn.GELU(),\n",
        "\n",
        "        nn.Conv2d(512, 512, kernel_size = 4, padding = 1, stride = 2, bias = False),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.GELU(),\n",
        "\n",
        "        nn.Conv2d(512, 1, kernel_size = 4, padding = 1, stride = 1, bias = False),\n",
        "        nn.Sigmoid(),\n",
        "    )\n",
        "    \n",
        "  def forward(self, inputs):\n",
        "    return self.model(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM2Dq1p7FAu6"
      },
      "source": [
        "class replay_buffer():\n",
        "    \n",
        "    def __init__(self, pool_size):\n",
        "      self.pool_size = pool_size\n",
        "      if self.pool_size > 0:  # create an empty pool\n",
        "        self.num_imgs = 0\n",
        "        self.images = []\n",
        "\n",
        "    def query(self, images):\n",
        "        to_return = []\n",
        "        for image in images:\n",
        "            image = torch.unsqueeze(image.data, 0)\n",
        "            if self.num_imgs < self.pool_size:   # if the buffer is not full; keep inserting current images to the buffer\n",
        "                self.num_imgs = self.num_imgs + 1\n",
        "                self.images.append(image)\n",
        "                to_return.append(image)\n",
        "            else:\n",
        "                p = random.uniform(0, 1)\n",
        "                if p > 0.5:  # by 50% chance, the buffer will return a previously stored image, and insert the current image into the buffer\n",
        "                    random_id = random.randint(0, self.pool_size - 1)  # randint is inclusive\n",
        "                    tmp = self.images[random_id].clone()\n",
        "                    to_return.append(tmp)\n",
        "                    self.images[random_id] = image\n",
        "                    \n",
        "                else:       # by another 50% chance, the buffer will return the current image\n",
        "                    to_return.append(image)\n",
        "        to_return = torch.cat(to_return, 0)   # collect all the images and return\n",
        "        return to_return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mAy8oYwN8Xb"
      },
      "source": [
        "def weights_init_normal(m):\n",
        "    \n",
        "    #classname will be something like: `Conv`, `BatchNorm2d`, `Linear`, etc.\n",
        "    classname = m.__class__.__name__\n",
        "    \n",
        "    #normal distribution with given paramters\n",
        "    std_dev = 0.02\n",
        "    mean = 0.0\n",
        "    \n",
        "    # Initialize conv layer\n",
        "    if hasattr(m, 'weight') and (classname.find('Conv') != -1):\n",
        "        torch.nn.init.normal_(m.weight.data, mean, std_dev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdeE28aNCDIt"
      },
      "source": [
        "# Set Network\n",
        "G_XtoY = Generator().to(device)\n",
        "G_YtoX = Generator().to(device)\n",
        "D_X = Discriminator().to(device)\n",
        "D_Y = Discriminator().to(device)\n",
        "\n",
        "#Weight initialization\n",
        "G_XtoY.apply(weights_init_normal)\n",
        "G_YtoX.apply(weights_init_normal)\n",
        "D_X.apply(weights_init_normal)\n",
        "D_Y.apply(weights_init_normal)\n",
        "\n",
        "print(\"                     G_XtoY                    \")\n",
        "print(\"-----------------------------------------------\")\n",
        "print(G_XtoY)\n",
        "print()\n",
        "\n",
        "print(\"                     G_YtoX                    \")\n",
        "print(\"-----------------------------------------------\")\n",
        "print(G_YtoX)\n",
        "print()\n",
        "\n",
        "print(\"                      D_X                      \")\n",
        "print(\"-----------------------------------------------\")\n",
        "print(D_X)\n",
        "print()\n",
        "\n",
        "print(\"                      D_Y                      \")\n",
        "print(\"-----------------------------------------------\")\n",
        "print(D_Y)\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaDzFsVUgjni"
      },
      "source": [
        "# Define loss function\n",
        "lambda_weight = 10\n",
        "\n",
        "loss_GAN = nn.BCEWithLogitsLoss().to(device)\n",
        "L1_loss = nn.L1Loss().to(device)\n",
        "\n",
        "def Cycle_loss(inputs, targets):\n",
        "  loss = L1_loss(inputs, targets)\n",
        "  return lambda_weight * loss\n",
        "\n",
        "def G_loss(inputs):\n",
        "  loss = loss_GAN(inputs, torch.ones_like(inputs))\n",
        "  return loss\n",
        "\n",
        "def D_loss(inputs, targets):\n",
        "  loss = loss_GAN(inputs, torch.ones_like(inputs))\n",
        "  \n",
        "  g_loss = loss_GAN(targets, torch.zeros_like(targets))\n",
        "  \n",
        "  total_loss = loss + g_loss\n",
        "\n",
        "  return total_loss * 0.5\n",
        "\n",
        "def Identity_loss(inputs,targets):\n",
        "  loss = L1_loss(inputs, targets)\n",
        "  return lambda_weight * loss * 0.5\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hLM6DM-OopA"
      },
      "source": [
        "lr = 0.001\n",
        "\n",
        "G_optimizer = torch.optim.Adam(list(G_XtoY.parameters()) + list(G_YtoX.parameters()), lr, betas=[0.500,0.999])\n",
        "D_X_optimizer = torch.optim.Adam(D_X.parameters(), lr , betas=[0.500,0.999])\n",
        "D_Y_optimizer = torch.optim.Adam(D_Y.parameters(), lr , betas=[0.500,0.999])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwwYW9R1TJ_V"
      },
      "source": [
        "def train(photo_dl, monet_dl, test, n_epochs=1000):\n",
        "    \n",
        "    D_losses = []\n",
        "    G_losses = []\n",
        "\n",
        "    test_X = next(iter(test))\n",
        "\n",
        "    batch_per_epoch = min(len(iter(photo_dl)),len(iter(monet_dl)))\n",
        "\n",
        "    D_avg_loss = 0\n",
        "    G_avg_loss = 0\n",
        "\n",
        "    photo_img = next(iter(photo_dl))\n",
        "    monet_img = next(iter(monet_dl))\n",
        "\n",
        "    Buffer_XtoY = replay_buffer(50)\n",
        "    Buffer_YtoX = replay_buffer(50)\n",
        "\n",
        "\n",
        "    #Loop through epochs\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "\n",
        "        if epoch % batch_per_epoch == 0:\n",
        "          photo_img = next(iter(photo_dl))\n",
        "          monet_img = next(iter(monet_dl))\n",
        "        \n",
        "        #move images to GPU if available (otherwise stay on CPU)\n",
        "        X = photo_img.to(device) # X\n",
        "        Y = monet_img.to(device) # Y\n",
        "        \n",
        "        # Discriminator Train\n",
        "        D_X_optimizer.zero_grad()\n",
        "\n",
        "        d_real_X = D_X(X)\n",
        "\n",
        "        fake_X = G_YtoX(Y).detach()\n",
        "\n",
        "        d_fake_X = D_X(fake_X)\n",
        "\n",
        "        fake_X = Buffer_YtoX.query(fake_X)\n",
        "        D_X_loss = D_loss(d_real_X,d_fake_X)\n",
        "        D_X_loss.backward()\n",
        "        D_X_optimizer.step()\n",
        "\n",
        "\n",
        "        D_Y_optimizer.zero_grad()\n",
        "\n",
        "        d_real_Y = D_Y(Y)\n",
        "\n",
        "        fake_Y = G_XtoY(X).detach()\n",
        "\n",
        "        d_fake_Y = D_Y(fake_Y)\n",
        "        \n",
        "        fake_Y = Buffer_XtoY.query(fake_Y)\n",
        "        D_Y_loss = D_loss(d_real_Y,d_fake_Y)\n",
        "        D_Y_loss.backward()\n",
        "        D_Y_optimizer.step()\n",
        "\n",
        "        D_total_loss = (D_X_loss + D_Y_loss) / 2\n",
        "\n",
        "        # Generator Train\n",
        "        # For domain Y\n",
        "        G_optimizer.zero_grad()\n",
        "\n",
        "        fake_X = G_YtoX(Y)\n",
        "\n",
        "        d_fake_X = D_X(fake_X)\n",
        "\n",
        "        G_YtoX_loss = G_loss(d_fake_X)\n",
        "\n",
        "        rec_Y = G_XtoY(fake_X)\n",
        "\n",
        "        Y_cycle_loss = Cycle_loss(Y, rec_Y)\n",
        "\n",
        "        Y_iden_loss = Identity_loss(Y,fake_X)\n",
        "\n",
        "        # For domain X\n",
        "        fake_Y = G_XtoY(X)\n",
        "\n",
        "        d_fake_Y = D_Y(fake_Y)\n",
        "\n",
        "        G_XtoY_loss = G_loss(d_fake_Y)\n",
        "\n",
        "        rec_X = G_YtoX(fake_Y)\n",
        "\n",
        "        X_cycle_loss = Cycle_loss(X, rec_X)\n",
        "\n",
        "        X_iden_loss = Identity_loss(X,fake_Y)\n",
        "\n",
        "\n",
        "        G_total_loss = G_YtoX_loss + G_XtoY_loss + Y_cycle_loss + X_cycle_loss + Y_iden_loss + X_iden_loss\n",
        "\n",
        "        G_total_loss.backward()\n",
        "        G_optimizer.step()\n",
        "        \n",
        "        # Train log\n",
        "\n",
        "        D_avg_loss += D_total_loss / batch_per_epoch\n",
        "        G_avg_loss += G_total_loss / batch_per_epoch\n",
        "\n",
        "        if epoch % batch_per_epoch == 0 :\n",
        "          D_losses.append(D_avg_loss.item())\n",
        "          G_losses.append(G_avg_loss.item())\n",
        "          real_epoch = int(epoch / batch_per_epoch)\n",
        "          total_epoch = int(n_epochs / batch_per_epoch)\n",
        "          print('Epoch [{:5d}/{:5d}] | D_total_loss: {:6.4f} | G_total_loss: {:6.4f}'.format(\n",
        "                    real_epoch, total_epoch, D_avg_loss.item(), G_avg_loss.item()))\n",
        "\n",
        "        if epoch % (batch_per_epoch * 10) == 0 :\n",
        "            G_XtoY.eval()\n",
        "            show_test(test_X,G_XtoY)\n",
        "            #set generators to train mode to continue training\n",
        "            G_XtoY.train()\n",
        "\n",
        "        if epoch % batch_per_epoch == 0:\n",
        "          D_avg_loss = 0\n",
        "          G_avg_loss = 0\n",
        "\n",
        "    return D_losses,G_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6XMaGjFMR8g"
      },
      "source": [
        "batch_per_epoch = min(len(photo_dl), len(monet_dl))\n",
        "epoch_true = 100\n",
        "n_epochs = epoch_true * batch_per_epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SSZy74pvDmXZ"
      },
      "source": [
        "%%time\n",
        "\n",
        "D_losses,G_losses = train(photo_dl,monet_dl,test,n_epochs = n_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pEn9JAa_6iCC"
      },
      "source": [
        "#Plot loss functions over training\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "D_losses = np.array(D_losses)\n",
        "G_losses = np.array(G_losses)\n",
        "plt.plot(D_losses, label='Discriminators', alpha=0.5)\n",
        "plt.plot(G_losses, label='Generators', alpha=0.5)\n",
        "plt.title(\"Training Losses\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Jjq83bmRC6uG"
      },
      "source": [
        "def save_checkpoint(model):\n",
        "  save_path = '/content/mount/MyDrive/dataset/Art_GAN/model/'\n",
        "  torch.save(model, save_path + 'monet_model.pt')  # 전체 모델 저장"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qDlmreV3CTHF"
      },
      "source": [
        "save_checkpoint(G_XtoY)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}